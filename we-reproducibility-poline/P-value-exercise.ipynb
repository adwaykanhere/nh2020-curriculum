{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Playing with P-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall some definitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $H_0$ : null hypothesis: The hypotheis that the effect we are testing for is null\n",
    "\n",
    "* $H_A$ : alternative hypothesis : Not $H_0$, so there is some signal\n",
    "\n",
    "* $T$ : The random variable that takes value \"significant\" or \"not significant\"\n",
    "\n",
    "* $T_S$ : Value of T when test is significant (eg $T = T_S$)\n",
    "\n",
    "* $T_N$ : Value of T when test is not significant (eg $T = T_N$)\n",
    "\n",
    "* $\\alpha$ : false positive rate - probability to reject $H_0$ when $H_0$ is true (therefore $H_A$ is false)\n",
    "\n",
    "* $\\beta$ : false negative rate - probability to accept $H_0$ when $H_A$ is true (i.e. $H_0$ is false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plainly: \n",
    "- a false positive is saying that something is true when it is false.\n",
    "- a false negative is saying that something is false when it is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is P-hacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-hacking is finding regular variations in either data or models and considering them to be significant. The result is a positive report that is not reproducible or does not generalize. It is often unintentional. In this notebook, we'll go through a few examples of how we can find (and subsequently report) statistically-significant results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Random selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll sample data points from a zero-mean Gaussian distribution and test whether the sample mean is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: Generate N samples from the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# define the Normal 0,1 object\n",
    "norm01 = sst.norm(0,1)\n",
    "# Let's fix the seed of the random generator\n",
    "#np.random.seed(42) # 42 is arbitrary \n",
    "# Draw from the normal:\n",
    "# norm01 has a \"random variables\" function drawing from this distribution\n",
    "# and returns a numpy array of a given size\n",
    "sample = norm01.rvs(size=(30,))\n",
    "plt.plot(sample,'+')\n",
    "sample = norm01.rvs(size=(30,))\n",
    "plt.plot(sample,'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the mean significant? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample mean and corrected standard deviation\n",
    "sample_mean = sample.mean()\n",
    "sample_std = sample.std()\n",
    "N = len(sample)\n",
    "\n",
    "std_corrected = np.sqrt((sample**2 - sample_mean**2).sum()/(N-1))\n",
    "\n",
    "# take into account the \"N-1\" since 1 degrees of freedoms have been used \n",
    "# to estimate the mean\n",
    "# assert not np.isclose(std_corrected, sample_std)\n",
    "assert np.isclose(std_corrected, np.sqrt(np.var(sample,ddof=1)))\n",
    "\n",
    "# t-test: compute t statistics\n",
    "t_value = sample_mean / (std_corrected/np.sqrt(N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# test it against the null : make a central t distribution \n",
    "central_t = sst.t(loc=0,scale=1,df=N-1)\n",
    "\n",
    "significance_thr = 0.05\n",
    "\n",
    "# use the survival function\n",
    "pvalue = central_t.sf(t_value)\n",
    "print(\"This is our p-value : {}\".format(pvalue))\n",
    "\n",
    "if pvalue < significance_thr:\n",
    "    print(\"Significant p-value!\")\n",
    "else:\n",
    "    print(\"Not good enough, continue p-hacking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the chance that our p-value is significant under the null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(norm01.rvs(size=(2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that generate a p-value when data are from N(0,1)\n",
    "\n",
    "def yield_a_pvalue(distrib, N=30):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    distrib:  distribution object(eg, norm(0,1))\n",
    "        a scipy.stats distribution \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A p-value\n",
    "    \"\"\"\n",
    "    sample = distrib.rvs(size=(N,))\n",
    "    sample_mean = sample.mean()\n",
    "    std_corrected = np.sqrt(np.var(sample, ddof=1))\n",
    "\n",
    "    # compute t statistics\n",
    "    t_value = sample_mean / (std_corrected/np.sqrt(N))\n",
    "    \n",
    "    return sst.t.sf(t_value, df=N-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On average: 20 \n",
    "nb_of_test_needed = 0\n",
    "while yield_a_pvalue(norm01) > significance_thr:\n",
    "    nb_of_test_needed += 1\n",
    "print(nb_of_test_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pval = 100\n",
    "pvalues = np.asarray([yield_a_pvalue(sst.norm(0,1)) for i in range(N_pval)])\n",
    "number_significant = (pvalues <= significance_thr).sum()\n",
    "print(\"We have {} tests significant over {} trials, ie {}%\"\n",
    "              .format(number_significant, N_pval, 100*number_significant/N_pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from non-zero mean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run tests that are similar to those above, but instead we'll examine the difference between effect size vs. signal-to-noise ratio (SNR).\n",
    "As above, we are trying to determine whether a sample mean is significantly different from zero. Compare modifying our effect size (the mean, 'm') and the noise ('sigma'). What do you notice if you keep the ratio, m/sigma constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def launch_a_bunch_of_tests(distrib, N_pval=1000, N=30):\n",
    "    \"\"\"\n",
    "    launches a series of sampling and then t tests on these (testing if the mean is > 0)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    distrib: the sampling distribution\n",
    "    N_pval: number of p-value to compute\n",
    "    N : the sample size\n",
    "    \"\"\"\n",
    "    pvalues = np.asarray([yield_a_pvalue(distrib) for i in range(N_pval)])\n",
    "    number_significant = (pvalues <= significance_thr).sum()\n",
    "    print(\"We have {} tests significant over {} trials, ie {}%\"\n",
    "                  .format(number_significant, N_pval, 100*number_significant/N_pval))\n",
    "\n",
    "\n",
    "# Demonstrate that the test depends only on the signal to noise ratio, not the effect size\n",
    "    \n",
    "# case 1: m=1.65, sigma=1\n",
    "#---------------------------\n",
    "loc, scale = (1.65/np.sqrt(N), 1)\n",
    "distrib = sst.norm(loc, scale)\n",
    "launch_a_bunch_of_tests(distrib)\n",
    "# case 2: m=0.165, sigma=0.1\n",
    "#---------------------------\n",
    "loc, scale = (0.165/np.sqrt(N), 0.1)\n",
    "distrib = sst.norm(loc, scale)\n",
    "launch_a_bunch_of_tests(distrib)\n",
    "#---------------------------\n",
    "# case 3: Modify 'm'; SNR is kept constant\n",
    "m = 50; sigma = m/1.65\n",
    "loc, scale = (m/np.sqrt(N), sigma)\n",
    "distrib = sst.norm(loc, scale)\n",
    "launch_a_bunch_of_tests(distrib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting refers to a model (e.g. a GLM) that fits too closely to the data on which it is trained and its predictions can't reproduced on new data.\n",
    "Overfitting can occur when there is very little data or if your model has too many parameters. For example, a GLM with 1000 explanatory variables can be might fit well to the time series of 4 subjects, but is unlikely to do well on a different set of 4 subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll create some data and use regression to fit a model. The important parts of the code are the \"modify\" section for you to modify and examine the resulting changes.\n",
    "The number of samples, \"n_samples\" is the amount of data you have.\n",
    "The number of model parameters, \"degrees\" is the number of degrees of freedom given to your model. It will perform a linear regression on our generated data and a polynomial of the given degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Define a function for us to fit; here we'll take a simple cosine\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "###\n",
    "# Modify\n",
    "###\n",
    "n_samples = 30  # Number of data points\n",
    "degrees = [1, 4, 15]   # Number of parameters; change the values to see the effect on out of sample error\n",
    "                       # For degree 4, the model will fit Y = b0 + b1*x + b2*x^2 + b3*x^3 + b4*x^4\n",
    "###\n",
    "\n",
    "test_max_value=1\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "X_test = np.linspace(0, test_max_value, 100)\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    train_score = mean_squared_error(y, pipeline.predict(X[:, np.newaxis]))\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    \n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, np.max(X_test)))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nSample Error: {:.2e}\\nOut of sample err. = {:.2e}(+/- {:.1e})\".format(\n",
    "        degrees[i], train_score.mean(), -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the three previous plots.\n",
    "The \"Sample error\" is the error for the data in your sample; it represents the error you would have if you evaluated your model on data it had already seen.\n",
    "The \"Out of sample error\" is the error when your model is evaluated on data it has not seen. It represents how well your model generalizes.\n",
    "In both cases, a lower error is better.\n",
    "\n",
    "1) The left-most plot presents a plausible model.\n",
    "\n",
    "2) The polynomial of degree 4 has the lowest out of sample error, indicating that it is the best-performing model.\n",
    "\n",
    "3) The high-degree polynomial has the lowest sample error, but a very large out of sample error, indicating that it does not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to select our high-degree polynomial and report those results, we can immediately see that our results could not be reproduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercises examined toy examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = pd.read_csv('brain_size.csv', delimiter=';', header=0, index_col=0)\n",
    "linear_regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's first look at our data:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is relatively simple, and contains 38 subjects with Full Scale, Verbal, and Perceptual IQ, as well as weight, height, and brain volume (\"MRI_Count\"). We'll repeat the previous exercise and we'll see a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Weight != '.']\n",
    "data = data[data.Height != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Weight\"] = pd.to_numeric(data[\"Weight\"])\n",
    "data[\"Height\"] = pd.to_numeric(data[\"Height\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Modify\n",
    "###\n",
    "num_subject = 30       # Number of subjects for in-sample data (must be less than 38)\n",
    "degrees = [1, 4, 15]   # For degree 4, the model will fit Y = b0 + b1*x + b2*x^2 + b3*x^3 + b4*x^4\n",
    "input_variable='Weight'\n",
    "predicted_variable='Height'\n",
    "###\n",
    "\n",
    "test_max_value=1\n",
    "\n",
    "X = np.array(data[input_variable][:num_subject]).T[:, np.newaxis]\n",
    "X = X[X != '.'].reshape(-1,1)\n",
    "y = np.array(data[predicted_variable][:num_subject]).T[:, np.newaxis]\n",
    "y = np.delete(y, np.where(X=='.')[0])\n",
    "\n",
    "X_test = np.array(data[input_variable][num_subject:]).T\n",
    "xind = np.argsort(X_test)\n",
    "X_test = np.take_along_axis(X_test, xind, 0)[:, np.newaxis]\n",
    "y_test = np.array(data[predicted_variable][num_subject:]).T\n",
    "y_test = np.take_along_axis(y_test, xind, 0)[:, np.newaxis]\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    train_score = mean_squared_error(y, pipeline.predict(X))\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    m_range = np.linspace(np.min(X), np.max(X), 100)[:, np.newaxis]\n",
    "    \n",
    "    plt.plot(m_range, pipeline.predict(m_range), label=\"Model\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test data\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((np.min(X), np.max(X_test)))\n",
    "    plt.ylim((np.min(y)*0.9, np.max(y)*1.1))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nSample Error: {:.2e}\\nOut of sample err. = {:.2e}(+/- {:.1e})\".format(\n",
    "        degrees[i], train_score.mean(), -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is caused by having a model that is too complex for the amount of available data. Such situations are sometimes unavoidable; however, there are ways of reducing the risks of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Acquire more data.\n",
    "\n",
    "The solution to a lot of modeling problems is 'more data'; however, that's rarely possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Reduce the degrees of freedom; use smaller models.\n",
    "\n",
    "Use the minimum number of explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use regulatization\n",
    "\n",
    "Regularization penalizes model parameters that don't sufficiently contribute to the explained variance. l1 regularization (aka Lasso) sets unimportant parameters to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Share your model.\n",
    "\n",
    "Ultimately, the generalizability of a method/claim will be determined by people being able to reproduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopting some of the practices discussed today help with producing good science: false positive results obtained by a incidental variations in data/methods can be quickly verified and rejected. Similarly, true positive results can be verified and trusted more easily."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
